
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/05_recorder_annealing.ipynb


from exp.nb_04 import *

import torch.nn.functional as F
import torch.nn as nn
from functools import partial

def create_learner(model_func, loss_func, data):
    return Learner(*model_func(data),loss_func, data)

def get_model_func(lr=0.5): return partial(get_model, lr=lr)

class Callback():

    _order = 0

    def set_runner(self, run):
        self.run = run

    def __getattr__(self, k):
        return getattr(self.run, k)

    @property
    def name(self):
        name = re.sub(r'Callback$', '', self.__class__.__name__) # removes Callback from custom callback class name
        return camel2snake(name or "callback")

class Recorder(Callback):
    def begin_fit(self):
        self.losses = []
        self.lrs = []

    def after_step(self):
        if not self.in_train: return # don't
        self.losses.append(self.loss.detach().cpu())
        self.lrs.append(self.opt.param_groups[-1]['lr'])

    def plot_losses(self):
        plt.plot(self.losses)

    def plot_lr(self):
        plt.plot(self.lrs)

class ParamScheduler(Callback):
    _order = 1

    def __init__(self, pname, sched_func):
        self.pname = pname
        self.sched_func = sched_func

    def set_param(self):
        for pg in self.opt.param_groups:
            pg[self.pname] = self.sched_func(self.n_epochs/self.epochs)

    def begin_batch(self):
        if self.in_train: self.set_param()



def annealer(f):
    def _inner(start, end): return partial(f, start, end)
    return _inner

@annealer
def sched_lin(start, end, pos): return start + (end -start) * pos


import math

@annealer
def sched_no(start, end, pos): return start

@annealer
def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2

@annealer
def sched_exp(start, end, pos): return start * (end/start) ** pos

def cos_1cycle_anneal(start, high, end):
    return [sched_cos(start, high), sched_cos(high, end)]

def combine_scheds(pcts, scheds):
    assert sum(pcts) == 1.
    pcts = tensor([0] + listify(pcts))
    assert torch.all(pcts>=0)
    pcts = torch.cumsum(pcts, 0)
    def _inner(pos):
        idx = (pos >= pcts).nonzero().max()
        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])
        return scheds[idx](actual_pos)
    return _inner