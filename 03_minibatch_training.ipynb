{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch Training\n",
    "> Basic training loop, Parameters, and Optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = y_train.max()+1;c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model and Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function: Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each grayscale image of a handwritten shape in our dataset has exactly 1 correct answer - an integer between 0 and 9 - these are called the labels or targets. \n",
    "\n",
    "Our `y_train` is a tensor of integers that map to the `x_train` images. \n",
    "\n",
    "We can index into the `y_train` to see the labels to images 0, 1, and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also think of these labels as one-hot encoded vectors of length 10 where the label corresponds to a 1 at the label's index and everything else is a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0 = torch.zeros(10)\n",
    "y0[y_train[0]] = 1\n",
    "y0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's output is simply a length 10 vector for every example that is the result of numerous matrix multiplications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0918,  0.0414,  0.1069, -0.1664,  0.0340,  0.1779,  0.0468, -0.0298,\n",
       "        -0.0210, -0.1646], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we would like is a probability distribution over each of our 10 classes: each class gets a probabilty, the highest corresponds to the class the model has learned is the most \"correct\". The model most therefore learn and adjust its parameters by quantifying how wrong its guess was.\n",
    "\n",
    "In order to accomplish this for our multi-class problem we'll use cross entropy loss.\n",
    "\n",
    "The first step is to scale the outputs by putting them through a softmax function:\n",
    "<p>\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "</p>\n",
    "\n",
    "This turns our length 10 output vector into a probability distribution.\n",
    "\n",
    "For example, for the numerator, to raise the first two rows to the e is just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0962, 1.0423, 1.1129, 0.8467, 1.0346, 1.1947, 1.0479, 0.9706, 0.9792,\n",
       "         0.8483],\n",
       "        [1.0869, 1.0400, 1.1389, 0.9027, 1.0509, 1.3027, 1.1103, 1.0629, 1.0749,\n",
       "         0.8000]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:2].exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator is trickier because we don't want to sum all of the rows together. We need divide each exponentiated value by its own row. \n",
    "\n",
    "Therefore, this won't work because it lumps everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.7437, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:2].exp().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we need to pass `keepdim=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.1734],\n",
       "        [10.5703]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:2].exp().sum(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here are the first two rows softmaxed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1077, 0.1025, 0.1094, 0.0832, 0.1017, 0.1174, 0.1030, 0.0954, 0.0963,\n",
       "         0.0834],\n",
       "        [0.1068, 0.1022, 0.1119, 0.0887, 0.1033, 0.1281, 0.1091, 0.1045, 0.1057,\n",
       "         0.0786]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_preds = pred[:2].exp() / pred[0].exp().sum(-1, keepdim=True); soft_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we sum a row we get 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_preds[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x.exp() / (x.exp().sum(-1, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_preds1 = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately refactor this by remembering that:\n",
    "\n",
    "<p>\n",
    "$$ \\log{\\frac{a}{b}} = \\log{a} - \\log{b} $$\n",
    "    \n",
    "</p>\n",
    "\n",
    "Therefore:\n",
    "\n",
    "<p>\n",
    "$$ \\displaystyle \\log{\\frac{e^x_i}{\\sum_{j=0}^{n-1} e^x_j}} = \\log{e^x} - \\log{\\sum_{j=0}^{n-1} e^x_j} = x  - \\log{\\sum_{j=0}^{n-1} e^x_j} $$\n",
    "</p>\n",
    "\n",
    "In code this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_preds = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(soft_preds, soft_preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an output vector of predictions, $\\hat{y}$, in the form of a probability distribution over the possible classes of $y$ (0-9) we can use cross entropy loss to caculate just how far off our prediction is from the target value.\n",
    "\n",
    "We're trying to find how dissimilar our prediction is to the target. So we are comparing the two distributions. \n",
    "\n",
    "Assuming our $y$ is one-hot encoded, we calculate the cross entropy loss for a single example by taking the dot product of the two vectors:\n",
    "\n",
    "<p>\n",
    "$$  L =  -y \\cdot \\log{\\hat{y}} $$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1419, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(y0 @ soft_preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can do this another way without having to one-hot encode our labels. \n",
    "\n",
    "We'll use integer array indexing - we can pass a list of integers for each dimension and get back those specific rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1419], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-soft_preds[[0],[y_train[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn that into a proper loss function which takes averages all of the negative loss logs over the entire output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3071, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(soft_preds, y_train); loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogSumExp Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "<p>\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "</p>\n",
    "    \n",
    "    \n",
    "where a is the maximum of the $x_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0] # grab the largest number in x\n",
    "    print(m)\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log() # subtract it out and then add it back in the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now built our loss function so let's go ahead and use the Pytorch version which combines `log_softmax` and `nll_loss` in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3070, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyloss = F.cross_entropy(pred, y_train); pyloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(pyloss, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the parts now to create an algorithm!\n",
    "\n",
    "The training loop is combines everything we have done so far into an interative process. We loop over the data again and again to fine-tune our model's parameters. \n",
    "\n",
    "Here is what we need the training loop to do:\n",
    "\n",
    "- Get a batch of inputs and pass them to the model to get a batch of outputs\n",
    "- Compute the loss of by comparing the outputs to the labels\n",
    "- Calculate the gradients of the loss function with respect to the model parameters\n",
    "- Finally update the parameters using those gradients and a learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice if we had some sort of metric to follow to see how many of the training examples we are getting correct. \n",
    "\n",
    "We'll start with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 5, 6])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pred, dim=1)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(yh, y): return (torch.argmax(yh, dim=1)==y).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a single mini-batch to test that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128                  # batch size\n",
    "xb = x_train[0:bs]     # a mini-batch from x\n",
    "preds = model(xb)      # predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us 128 predictions - each prediction here being a vector of length 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0918,  0.0414,  0.1069, -0.1664,  0.0340,  0.1779,  0.0468, -0.0298,\n",
       "         -0.0210, -0.1646], grad_fn=<SelectBackward>),\n",
       " torch.Size([128, 10]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put these predictions through our loss function with our labels and get some sort of measurement as to how far off they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3045, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "loss = loss_func(preds, yb); loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0469)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 10% accuracy, that's basically choosing randomly.\n",
    "\n",
    "We have a long way to go but at least everything appears to be working.\n",
    "\n",
    "Now let's look at our model's weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0302,  0.0338, -0.0123,  ..., -0.0202,  0.0214,  0.0049],\n",
       "        [-0.0146, -0.0077,  0.0349,  ...,  0.0049,  0.0070,  0.0019],\n",
       "        [ 0.0141,  0.0241, -0.0318,  ..., -0.0278,  0.0213, -0.0353],\n",
       "        ...,\n",
       "        [-0.0132, -0.0237, -0.0061,  ...,  0.0226, -0.0165, -0.0246],\n",
       "        [ 0.0126, -0.0019,  0.0001,  ...,  0.0199,  0.0091, -0.0242],\n",
       "        [ 0.0325, -0.0055, -0.0159,  ...,  0.0244,  0.0131,  0.0208]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage the gradients with respect to the loss have not been computed. \n",
    "\n",
    "Pytorch only computes them once `.backward` is called. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6134e-05,\n",
       "        3.1168e-05, 2.7501e-05, 1.4667e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7038e-05, 1.6612e-04,\n",
       "        3.3224e-04, 4.4549e-04, 3.6706e-04, 1.2165e-04])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weight.grad[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally begin to train. \n",
    "\n",
    "Three hyperparameters we need to set are the batch size, the learning rate and the number of epochs (the number of times we iterate through the entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "lr = 0.5\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0,n,bs):\n",
    "        xb = x_train[i:i+bs]\n",
    "        yb = y_train[i:i+bs]\n",
    "        loss = loss_func(model(xb), yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= lr * l.weight.grad\n",
    "                    l.bias   -= l.bias.grad   * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias  .grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1821, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's >90% accuracy. Not terrible for a simple neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9316)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_train[:512]), y_train[:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by no longer treating our Relu as a separate layer. Instead we'll use `F.relu` which is the functional form that returns activations. \n",
    "\n",
    "We then have two linear layers from `nn` these linear layers are automatically registered by the `nn.Module` class as the parameters of the model. \n",
    "\n",
    "We can call `model.parameters()` now and it will return a generator that does essentially what we were doing manually by iterating through the list of layers and checking for the `weight` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look inside our model.\n",
    "\n",
    "We can do this by calling the `.named_children` method on the model.\n",
    "\n",
    "\n",
    "    Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 : Linear(in_features=784, out_features=64, bias=True)\n",
      "l2 : Linear(in_features=64, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for l in model.named_children(): print(f\"{l[0]} : {l[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch's `nn.Module` has `__repr__` defined as the following:\n",
    "\n",
    "```python\n",
    "def __repr__(self):\n",
    "        # We treat the extra repr like the sub-module, one item per line\n",
    "        extra_lines = []\n",
    "        extra_repr = self.extra_repr()\n",
    "        # empty string will be split into list ['']\n",
    "        if extra_repr:\n",
    "            extra_lines = extra_repr.split('\\n')\n",
    "        child_lines = []\n",
    "        for key, module in self._modules.items():\n",
    "            mod_str = repr(module)\n",
    "            mod_str = _addindent(mod_str, 2)\n",
    "            child_lines.append('(' + key + '): ' + mod_str)\n",
    "        lines = extra_lines + child_lines\n",
    "\n",
    "        main_str = self._get_name() + '('\n",
    "        if lines:\n",
    "            # simple one-liner info, which most builtin Modules will use\n",
    "            if len(extra_lines) == 1 and not child_lines:\n",
    "                main_str += extra_lines[0]\n",
    "            else:\n",
    "                main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n",
    "\n",
    "        main_str += ')'\n",
    "        return main_str`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (l2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that we can call which will run our training loop. \n",
    "\n",
    "This is standard for Machine Learning libraries like Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def fit():\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0,n,bs):\n",
    "            end = i+bs if i+bs < n else n\n",
    "            xb = x_train[i:end]\n",
    "            yb = y_train[i:end]\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3917, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9116)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_valid), y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How does Pytorch know what attributes in `__init__` to set as the model parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python every time an attribute is assigned during a class initialization, `__setattr__()` is called. \n",
    "\n",
    "When we inherit from `nn.Module` and then execute `super().__init__()` Pytorch creates the following 'private' attributes: \n",
    "```python\n",
    "        self.training = True\n",
    "        self._parameters = OrderedDict()\n",
    "        self._buffers = OrderedDict()\n",
    "        self._backward_hooks = OrderedDict()\n",
    "        self._forward_hooks = OrderedDict()\n",
    "        self._forward_pre_hooks = OrderedDict()\n",
    "        self._state_dict_hooks = OrderedDict()\n",
    "        self._load_state_dict_pre_hooks = OrderedDict()\n",
    "        self._modules = OrderedDict()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is instantiated from the `Model` class and `self.l1 = nn.Linear` is set as in attribute `__setattr__()` is called. \n",
    "\n",
    "Pytorch then does the following:\n",
    "\n",
    "- checks if the attribute is a Parameter or a Module\n",
    "- checks to make sure that `nn.Module` `__init__` was called\n",
    "- then registers the Parameter or (sub)Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "\n",
    "class Parameter\n",
    "\n",
    "    '''A kind of Tensor that is to be considered a module parameter.\n",
    "    \n",
    "    Parameters are ~torch.Tensor subclasses, that have a very special property when used with Module s - when they're assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in ~Module.parameters iterator. Assigning a Tensor doesn't have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too.\n",
    "\n",
    "Arguments: data (Tensor): parameter tensor. requires_grad (bool, optional): if the parameter requires gradient. See excluding-subgraphs for more details. Default: True'''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Module\n",
    "    \n",
    "    '''Base class for all neural network modules.\n",
    "\n",
    "    Your models should also subclass this class.\n",
    "    \n",
    "    Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:'''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can demonstrate this by building a dummy module that has a dictionary called `_modules`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModule():\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "    \n",
    "    def __setattr__(self, k,v):\n",
    "        if not k.startswith(\"_\"): # register any keys that do not start with '_'\n",
    "            self._modules[k] = v # put it inside modules dict\n",
    "        super().__setattr__(k,v)\n",
    "    \n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            for p in l.parameters(): yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = DummyModule(m, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=64, bias=True), 'l2': Linear(in_features=64, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([64, 784]),\n",
       " torch.Size([64]),\n",
       " torch.Size([10, 64]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.shape for o in mdl.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we wanted to use the layers approach that we wrote earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__() # sets up the parameters module dict and other dicts\n",
    "            self.layers = layers\n",
    "            for i,l in enumerate(self.layers):\n",
    "                self.add_module(f'l{i}',l)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            for l in self.layers: x = l(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l0): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (l1): ReLU()\n",
       "  (l2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we insist on using the layers technique we can use the built in Pytorch `nn.ModuleList`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__() \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    def forward(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.4165, grad_fn=<NllLossBackward>), tensor(0.8750))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even easier would be to use the Pytorch `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3879, grad_fn=<NllLossBackward>), tensor(0.8750))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refactor our optimization step. \n",
    "\n",
    "In our training loop we called backward on the loss to compute the gradients and then to make the actual updates to the weights we did the following:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "\n",
    "We can simplify this bit of the loop if we instead put this away into an `Optimizer` class which will then have two separate methods: \n",
    "\n",
    "```python\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, model, lr):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):        \n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters():\n",
    "                p -= p.grad * self.lr\n",
    "        \n",
    "    def zero(self):\n",
    "        self.model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0,n,bs):\n",
    "        end = i+bs if i+bs < n else n\n",
    "        xb = x_train[i:end]\n",
    "        yb = y_train[i:end]\n",
    "        loss = loss_func(model(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2990, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the Pytorch version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optim.SGD.step` iterates through each `param_group` and then again through each group's `params` key which contain the layers in that group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[ 0.0242, -0.0223,  0.0028,  ..., -0.0136, -0.0251,  0.0132],\n",
       "           [ 0.0249,  0.0300,  0.0280,  ..., -0.0090,  0.0227, -0.0340],\n",
       "           [ 0.0076,  0.0300,  0.0173,  ...,  0.0251, -0.0326, -0.0252],\n",
       "           ...,\n",
       "           [-0.0334,  0.0330, -0.0164,  ...,  0.0063,  0.0304,  0.0350],\n",
       "           [-0.0112,  0.0052, -0.0064,  ..., -0.0089,  0.0333,  0.0117],\n",
       "           [ 0.0170,  0.0224, -0.0226,  ...,  0.0107,  0.0014,  0.0281]],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 0.0695, -0.1221, -0.2036,  0.0419, -0.4452, -0.0561, -0.0798, -0.1187,\n",
       "           -0.3741,  0.0679, -0.1101, -0.1773, -0.3565, -0.2370, -0.0692, -0.1396,\n",
       "           -0.2085, -0.3121, -0.2558, -0.4352, -0.1824, -0.6495, -0.0619, -0.0654,\n",
       "           -0.5264, -0.1799, -0.5893, -0.1687, -0.0630, -0.0676, -0.0492, -0.2636,\n",
       "           -0.1226, -0.1610, -0.3492,  0.1063, -0.2487,  0.0685, -0.0983, -0.2556,\n",
       "           -0.0129, -0.2272, -0.1577, -0.4540,  0.1023, -0.3471, -0.4983, -0.3630,\n",
       "           -0.3642, -0.3852, -0.0962, -0.6883, -0.0752, -0.3145, -0.3133,  0.1807,\n",
       "            0.0232, -0.8394, -0.3073, -0.1410, -0.0365, -0.0309, -0.1918, -0.1322],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[-5.8946e-01, -9.9541e-01, -6.1009e-01, -4.2944e-01, -1.7829e+00,\n",
       "            -6.1810e-01, -6.1268e-01, -1.7368e+00, -1.0910e+00, -2.5996e-01,\n",
       "            -1.2900e+00, -7.7910e-01, -2.1162e-01, -8.9763e-01, -4.3329e-01,\n",
       "            -6.6195e-02, -1.7197e+00, -2.4720e-01, -1.7300e+00, -5.7200e-01,\n",
       "            -3.8469e-01, -7.8608e-01, -5.5976e-01, -6.5411e-01, -1.3015e+00,\n",
       "            -1.0755e+00, -1.7504e+00, -1.4150e+00, -6.3110e-01, -1.2771e-01,\n",
       "            -7.4116e-01, -1.3314e+00, -8.8672e-01, -1.3139e+00, -8.2490e-01,\n",
       "            -2.8461e-01, -4.9979e-01, -6.2796e-01, -1.4646e+00, -9.5640e-01,\n",
       "            -7.2762e-01, -6.6643e-02, -4.1593e-01, -1.9768e+00, -3.8755e-01,\n",
       "             1.4441e-01, -1.1516e+00, -1.4967e+00,  4.4487e-02, -2.0674e+00,\n",
       "            -7.0269e-01, -6.4916e-01, -2.8970e-01, -9.1833e-01, -6.1617e-01,\n",
       "             1.4324e+00, -5.0992e-01, -6.8946e-01, -7.7389e-01, -6.2576e-01,\n",
       "            -5.6817e-01, -2.1706e-01, -5.1434e-01, -8.3437e-01],\n",
       "           [ 1.4736e-01,  6.1899e-01,  3.8367e-01,  2.0474e-01,  1.6234e-01,\n",
       "             1.9910e-02,  5.2521e-01,  2.0243e-01,  2.6336e-01,  4.8058e-01,\n",
       "             1.5212e-02,  2.9719e-01,  1.0465e-01,  3.9997e-01,  4.9179e-01,\n",
       "            -1.0686e-02, -6.3055e-02,  4.4104e-03,  6.9422e-01,  2.3276e-02,\n",
       "             2.0056e-01,  3.2635e-01, -1.3552e-01,  3.9350e-01,  1.8209e-01,\n",
       "            -1.5804e-02,  1.9072e-01,  6.3875e-01,  1.3378e-02,  7.5685e-01,\n",
       "            -2.3055e-01,  2.4656e-01,  5.6902e-03,  2.5361e-01,  4.1348e-01,\n",
       "             4.6413e-01, -4.2637e-03, -2.1373e-01,  2.5735e-01,  1.3779e-01,\n",
       "             1.3182e-01, -2.4420e-02,  8.3810e-02,  5.6315e-01, -3.9571e-01,\n",
       "             9.7058e-02,  7.2392e-02, -4.8051e-02,  6.7091e-02,  1.9506e-01,\n",
       "             4.7850e-01, -3.3020e-02, -9.9602e-02,  3.5028e-01,  4.6059e-01,\n",
       "            -7.0152e-01,  4.7127e-01,  1.1147e-01,  2.8727e-01,  2.7892e-01,\n",
       "            -3.2688e-01,  5.3405e-01, -7.7136e-02,  2.4380e-01],\n",
       "           [ 4.0262e-01,  5.9595e-01, -4.2393e-01,  4.8300e-01, -1.3116e-01,\n",
       "             9.7536e-03,  9.1390e-02, -2.7039e-01,  1.5303e-01, -5.3912e-01,\n",
       "            -4.6389e-01, -4.1841e-01, -4.4547e-01, -1.1236e-01, -3.9071e-01,\n",
       "             9.8153e-02, -4.9088e-01, -6.0300e-01, -6.4100e-01, -4.9705e-01,\n",
       "             3.4840e-01, -1.1200e+00, -2.0423e-01,  1.0177e+00, -9.5590e-01,\n",
       "            -4.3663e-01, -1.1467e+00, -5.8674e-02, -2.6659e-01, -1.6858e-01,\n",
       "             2.0244e-02, -2.0337e-01,  1.6642e-01, -3.7115e-02, -6.8630e-01,\n",
       "             8.4875e-02, -6.8238e-02,  9.7729e-01,  1.4652e-01,  5.0473e-01,\n",
       "            -4.2074e-01, -4.3144e-01, -2.6734e-01, -5.8175e-01, -2.7036e-02,\n",
       "            -6.0768e-01, -9.1842e-01, -5.3203e-01, -4.9638e-01, -4.7890e-02,\n",
       "             1.8399e-01, -9.6529e-01, -2.9659e-02, -6.1856e-01, -5.7741e-01,\n",
       "            -6.6060e-02,  2.8556e-01, -1.2753e+00, -5.6649e-01,  5.9104e-01,\n",
       "             4.1077e-01, -4.5074e-02,  3.8024e-01, -4.5037e-01],\n",
       "           [-3.4428e-01, -1.0206e-01,  2.9607e-01, -2.7795e-01,  1.2728e-01,\n",
       "            -1.9542e-02,  1.0291e-01,  3.2123e-01,  5.2703e-01,  2.4541e-01,\n",
       "             1.4009e-01,  7.8937e-01,  4.4860e-01,  4.9172e-01, -9.6546e-02,\n",
       "             6.3077e-02,  3.0765e-01, -1.3423e-02,  2.5715e-01,  3.6962e-01,\n",
       "            -2.0634e-01,  7.7443e-01,  6.7656e-01,  5.8094e-01,  7.5473e-01,\n",
       "             3.5015e-01,  5.9438e-01, -6.5608e-02, -3.5443e-01, -4.7868e-01,\n",
       "            -5.5013e-02,  1.6557e-01,  3.7571e-01,  3.1492e-01,  6.6933e-01,\n",
       "            -4.0002e-01, -6.3331e-03,  1.8645e-01, -3.0906e-01,  3.5314e-01,\n",
       "             1.1049e-02,  6.4653e-01,  2.7413e-01,  1.9415e-01,  3.1959e-01,\n",
       "             2.0268e-01,  7.6408e-02,  3.0792e-01,  2.5554e-01,  6.0842e-01,\n",
       "            -8.5416e-02,  9.1566e-01,  2.6886e-02,  1.5939e-02,  3.4539e-01,\n",
       "            -1.2341e-01,  4.3908e-02,  8.9834e-01,  6.8286e-01,  5.7730e-01,\n",
       "             5.5149e-01,  1.5502e-01, -1.0673e-01, -5.5822e-02],\n",
       "           [ 7.0161e-01, -2.3185e-01,  6.5047e-04, -2.0168e-01,  5.5600e-01,\n",
       "            -4.4161e-01, -1.6711e-01,  3.2967e-01, -1.3243e-01, -2.8431e-01,\n",
       "            -9.2141e-02,  8.8790e-02,  6.1324e-02,  3.6411e-01,  4.3315e-02,\n",
       "             1.8284e-02,  5.3717e-01,  1.3777e-01,  1.6545e-01,  3.1119e-01,\n",
       "             2.7656e-01,  6.6881e-02, -2.6726e-01, -4.3709e-01,  4.8675e-01,\n",
       "             6.8670e-02,  7.0873e-01, -1.3129e-01,  7.4835e-01,  3.6377e-02,\n",
       "             4.3994e-01,  4.1828e-01,  3.0104e-02,  4.7315e-01, -1.7206e-02,\n",
       "            -2.2307e-01,  4.4726e-01,  5.2594e-01,  5.9244e-01, -3.3652e-01,\n",
       "             4.2258e-01, -3.2600e-01, -1.1081e-01,  4.4143e-01, -8.5929e-01,\n",
       "            -2.9924e-03,  6.4564e-01,  1.1503e+00, -1.4358e-02,  1.8174e-01,\n",
       "             4.0991e-03,  3.5616e-01, -5.7668e-01,  2.3412e-01, -6.7599e-02,\n",
       "            -9.3872e-02,  5.4544e-01,  6.1371e-02,  7.4929e-02, -1.0204e-01,\n",
       "            -1.2033e-01,  1.0361e-02,  4.8076e-02, -4.3451e-01],\n",
       "           [-2.8586e-01, -1.0493e-01,  1.5313e-01,  5.1876e-01, -2.7053e-02,\n",
       "             2.3112e-01, -2.2397e-02, -4.4221e-01,  1.8422e-01,  1.0377e+00,\n",
       "             6.4909e-01,  8.7938e-01,  2.5460e-01, -8.8508e-01,  1.6931e-01,\n",
       "            -2.4048e-02, -1.1946e-01,  2.0680e-01,  3.1130e-01,  6.5575e-02,\n",
       "            -5.3606e-01,  4.9214e-01,  9.3470e-01, -1.2098e-01,  3.6280e-01,\n",
       "            -9.6916e-02,  4.7032e-01,  1.7976e-01, -4.9625e-02, -2.8532e-02,\n",
       "            -6.5478e-01,  1.2713e-01,  1.4377e-01,  1.0490e-01,  2.0465e-01,\n",
       "             9.6936e-01, -9.6575e-04,  4.1975e-02, -1.5567e-02, -5.7961e-01,\n",
       "             4.5192e-01, -4.6537e-02,  1.0483e-01,  5.3052e-01,  8.8192e-01,\n",
       "             5.4756e-03,  2.5424e-01,  2.8760e-01, -5.3971e-03,  2.0757e-01,\n",
       "             3.2294e-02,  8.1343e-02,  6.6076e-02,  4.6617e-01,  6.1208e-02,\n",
       "            -3.6865e-01, -2.9696e-01,  5.9388e-01,  3.7265e-01, -2.7830e-01,\n",
       "             1.9883e-01,  1.6172e-01, -2.1520e-03,  9.0075e-01],\n",
       "           [-9.1282e-02,  4.0880e-01,  8.2211e-02,  3.0096e-01,  3.7115e-01,\n",
       "            -9.8904e-02, -2.1929e-01, -1.0130e-01, -3.6477e-01,  1.3295e-02,\n",
       "            -4.1978e-02, -4.8751e-01,  2.0952e-01, -2.7447e-01,  2.2419e-01,\n",
       "            -8.7202e-02,  5.7381e-01,  3.2917e-01,  4.6564e-01,  5.6258e-02,\n",
       "            -2.4680e-02,  3.2448e-01, -3.9930e-01, -3.8917e-01,  1.1917e-01,\n",
       "             6.8916e-01,  3.6015e-01,  5.6923e-01,  2.6191e-01, -9.5089e-02,\n",
       "             1.0289e-01,  7.6509e-01,  3.7471e-01, -5.8906e-02,  3.1100e-02,\n",
       "             7.6729e-02,  5.6047e-01, -1.4122e-01,  1.0862e+00, -1.5297e-01,\n",
       "            -6.3105e-01,  1.3481e-01,  1.4453e-01,  8.2898e-01, -3.8684e-01,\n",
       "            -3.4918e-02,  6.7828e-01, -1.3000e-02,  1.0571e-01,  7.2519e-01,\n",
       "             1.2002e-01,  9.8705e-02,  9.7753e-01,  7.2115e-01, -5.7575e-02,\n",
       "             1.0517e-01,  6.6588e-02,  5.9058e-01, -4.9168e-02, -2.9221e-01,\n",
       "             1.8939e-01,  6.1547e-02,  5.0224e-01,  7.5143e-01],\n",
       "           [ 4.1170e-01,  1.8943e-01,  8.4494e-02, -4.3106e-02,  3.9995e-01,\n",
       "             9.0398e-01,  3.0760e-01,  6.2089e-01, -3.3350e-01, -4.0979e-01,\n",
       "             1.1549e+00, -1.8415e-01,  7.0240e-02,  3.7650e-01,  7.4481e-02,\n",
       "             9.1855e-02,  1.1725e+00, -6.7355e-02,  7.0285e-01,  6.1826e-01,\n",
       "             2.8183e-01,  4.5938e-02,  1.9428e-01,  7.8283e-01,  5.6588e-01,\n",
       "             4.8374e-01,  3.0270e-01,  3.0356e-02,  4.4761e-01,  9.5450e-02,\n",
       "             3.5232e-01, -1.4289e-02,  2.1719e-01,  4.4987e-01,  1.1990e-01,\n",
       "             6.2204e-03, -4.1448e-02, -1.6078e-01,  3.0546e-02,  3.2224e-01,\n",
       "            -2.7919e-01,  3.4560e-01,  5.0986e-02, -6.4437e-02,  5.2357e-01,\n",
       "            -2.4402e-02,  5.2329e-01,  5.7898e-01,  5.6708e-01, -3.8952e-02,\n",
       "            -6.0671e-02,  3.7673e-01, -2.2042e-02,  5.4958e-02,  4.3768e-01,\n",
       "            -3.5846e-01, -2.2302e-02,  2.4984e-01,  3.2075e-01, -1.8855e-01,\n",
       "             1.9209e-01, -1.0522e-01,  2.1145e-01,  1.6874e-01],\n",
       "           [-8.1021e-01,  2.2114e-01,  1.8591e-01, -8.1978e-02,  4.9352e-01,\n",
       "            -1.9985e-01,  6.9653e-02, -4.3725e-02,  8.2376e-01, -6.6659e-01,\n",
       "            -1.8166e-02, -1.3726e-01,  2.5030e-01,  1.6389e-02,  7.7665e-03,\n",
       "            -1.5684e-02,  9.6094e-03, -3.1759e-02,  2.6580e-01,  1.8628e-01,\n",
       "             5.9159e-01,  6.8857e-01, -5.3471e-01, -4.5519e-01,  3.4545e-01,\n",
       "             7.2116e-02,  6.4954e-01,  3.2246e-01, -2.8353e-01, -1.6343e-01,\n",
       "            -3.6800e-01,  3.6197e-01,  2.6918e-03,  1.8404e-01,  3.9387e-01,\n",
       "            -6.0155e-01, -6.1931e-02, -4.9957e-01, -1.6377e-01,  5.7892e-01,\n",
       "             7.9558e-01,  2.6996e-01,  2.1753e-01,  4.7850e-01,  5.7942e-02,\n",
       "             2.3656e-01,  4.3513e-01,  1.6962e-01,  2.4515e-02,  5.4271e-01,\n",
       "             3.2875e-01,  3.7127e-01, -1.6970e-01,  1.6154e-01,  2.3152e-01,\n",
       "             9.4109e-02, -4.7913e-01,  4.6616e-01,  4.3372e-01, -4.2300e-02,\n",
       "            -8.9089e-02, -1.3694e-01,  3.9908e-01, -1.6736e-01],\n",
       "           [ 9.2075e-01, -5.6832e-02, -2.8283e-01, -6.8762e-01, -2.2668e-01,\n",
       "             3.4088e-03, -2.3074e-01,  1.0204e+00,  2.2800e-01,  4.2704e-01,\n",
       "            -3.4855e-01,  8.0587e-02, -1.7857e-01,  3.5792e-01, -1.3834e-01,\n",
       "            -2.4196e-01, -3.3001e-01, -2.1850e-01, -2.4889e-01, -5.5780e-01,\n",
       "            -3.3461e-01, -4.5469e-01,  4.0390e-01, -4.9752e-01, -3.5803e-01,\n",
       "            -2.7136e-01, -5.5564e-01, -2.8440e-01,  2.3450e-01,  1.3883e-01,\n",
       "             7.6729e-01, -3.0211e-01,  3.9496e-02, -3.4222e-01, -3.5777e-01,\n",
       "            -2.4313e-02, -1.4975e-01, -2.1483e-01, -9.4760e-02,  5.4784e-02,\n",
       "             3.8679e-01, -2.7771e-01, -2.2867e-01, -6.5055e-01,  1.1780e-01,\n",
       "            -3.0037e-01, -3.5958e-01, -5.4361e-01, -3.9687e-01, -1.7567e-01,\n",
       "            -2.7514e-01, -5.5101e-01, -4.3651e-01, -2.8556e-01, -3.7651e-01,\n",
       "             1.3694e-01, -3.5114e-01, -6.7469e-01, -4.7721e-01, -2.8824e-01,\n",
       "            -2.7694e-01,  6.9140e-03, -6.8546e-01, -2.0507e-01]],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 1.3408, -0.5579, -0.0957, -0.4227, -0.1308,  0.3742, -0.3239, -0.4748,\n",
       "            0.1084,  0.1185], requires_grad=True)],\n",
       "  'lr': 0.5,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a quick function to get model and an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0,n,bs):\n",
    "        end = i+bs if i+bs < n else n\n",
    "        xb=x_train[i:end]\n",
    "        yb=y_train[i:end]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1690, grad_fn=<NllLossBackward>), tensor(0.9511))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpreds = model(x_valid)\n",
    "loss,acc = loss_func(vpreds, y_valid), accuracy(vpreds, y_valid)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another part of our training loop we can improve is:\n",
    "\n",
    "```python\n",
    "    for i in range(0,n,bs):\n",
    "        end = i+bs if i+bs < n else n\n",
    "        xb=x_train[i:end]\n",
    "        yb=y_train[i:end]\n",
    "```\n",
    "\n",
    "Let's build a `Dataset` class that will hold the `x`'s and `y`'s in one object. \n",
    "\n",
    "```python\n",
    "xb, yb = train_ds[i:end]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x,y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self,key): \n",
    "        return self.x[key], self.y[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_ds) == len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = train_ds[0:5]\n",
    "assert xb.shape==(5,28*28)\n",
    "assert yb.shape==(5,)\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0,n,bs):\n",
    "        end = i+bs if i+bs < n else n\n",
    "        xb, yb = train_ds[i:end]\n",
    "        loss = loss_func(model(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2110, grad_fn=<NllLossBackward>), tensor(0.9360))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpreds = model(x_valid)\n",
    "loss,acc = loss_func(vpreds, y_valid), accuracy(vpreds, y_valid)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use the same refactoring logic to make it so we can simply pull `x` and `y` batches out of a `DataLoader` class that holds the `Dataset` class we just made. \n",
    "\n",
    "```python\n",
    "for xb, yb in train_dl:\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.ds/self.bs)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): \n",
    "            yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, 64)\n",
    "valid_dl = DataLoader(valid_ds, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xb.shape == (128, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26d50eaa430>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1685, grad_fn=<NllLossBackward>), tensor(0.9529))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpreds = model(x_valid)\n",
    "loss,acc = loss_func(vpreds, y_valid), accuracy(vpreds, y_valid)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For certain datasets where the dependent variable is in a specific order we would want to shuffle the data before loading it into batches and putting it through the model. \n",
    "\n",
    "Every epoch we'll iterate through the entire dataset randomly. The model will get to see each training example once but the order will be different each time. \n",
    "\n",
    "To do this we need our `Dataloader` to send a batch size of random integers that are within the range of the dataset. \n",
    "\n",
    "We'll need permutations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 7, 0, 4, 5, 2, 6, 1, 8, 9])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "        self.shuffle = shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            perms = torch.randperm(len(self.ds))\n",
    "            for i in range(0, len(perms), bs): yield self.ds[perms[i:self.bs]]\n",
    "        else:\n",
    "            for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(train_ds, 128, shuffle=True)\n",
    "test_valid_dl = DataLoader(valid_ds, 128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26d59a1c1c0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(test_dl))\n",
    "plt.imshow(xb[0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26d59a63280>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(test_dl))\n",
    "plt.imshow(xb[0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26d59abc850>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(test_dl))\n",
    "plt.imshow(xb[0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1671, grad_fn=<NllLossBackward>), tensor(0.9487))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpreds = model(x_valid)\n",
    "loss,acc = loss_func(vpreds, y_valid), accuracy(vpreds, y_valid)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampler v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n = len(ds) # just the length of the dataset not the whole dataset\n",
    "        self.bs = bs\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = Dataset(*train_ds[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this out let's see when `shuffle=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(small_ds, 3, False)\n",
    "[x for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([7, 1, 4]), tensor([9, 2, 0]), tensor([8, 5, 6]), tensor([3])]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(small_ds, 3, True)\n",
    "[x for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds = ds\n",
    "        self.sampler = sampler\n",
    "        self.collate_fn = collate_fn\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26d59b12e20>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAANvUlEQVR4nO3dbYxc5XnG8euCEgnbEWAMxiXGTiMsgSpBioUqJbyUKMZF4k0iJRbClCItH0yJpYrWSkGxKJUQbWoBHyJtZMBFKSGSsQKo4CAr4PZLxILwOwlvxnG8snER4u1Din33wx5XG7PznGVmzpyx7/9PWs3suefMuX3ka8+ZeebM44gQgOPfCW03AGAwCDuQBGEHkiDsQBKEHUjijwa5Mdu89Q80LCI81fKejuy2l9r+te03ba/q5bkANMvdjrPbPlHSbyR9W9JeSS9LWhYROwvrcGQHGtbEkf1iSW9GxNsR8XtJP5V0bQ/PB6BBvYT9bEm/nfT73mrZH7A9YnvM9lgP2wLQo17eoJvqVOFzp+kRMSppVOI0HmhTL0f2vZLmT/r9K5L29dYOgKb0EvaXJZ1r+6u2vyTpu5Ke7k9bAPqt69P4iPjM9h2SNko6UdIjEbGjb50B6Kuuh9662hiv2YHGNfKhGgDHDsIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEl3Pzy5JtndL+kjSIUmfRcTifjQFoP96CnvlLyLiYB+eB0CDOI0Hkug17CHpF7ZfsT0y1QNsj9gesz3W47YA9MAR0f3K9h9HxD7bZ0p6QdLfRsTmwuO73xiAaYkIT7W8pyN7ROyrbg9I2iDp4l6eD0Bzug677Zm2v3zkvqQlkrb3qzEA/dXLu/FzJW2wfeR5/iMinu9LV8A0nHHGGcX6unXrOtZ27NhRXHf16tXF+ieffFKsD6Ouwx4Rb0u6oI+9AGgQQ29AEoQdSIKwA0kQdiAJwg4k0Y8LYYBG1A2tbdy4sVi/4ILOg0VLliwprnvKKacU6yMjU346fKhxZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJHr6ppovvDG+qQaTLF++vFi/6667ivXzzjuvWK8uv55S3f/79957r1hfunRpsb5ly5ZivUmNfFMNgGMHYQeSIOxAEoQdSIKwA0kQdiAJwg4kwfXsKJo5c2axfumllxbrDz74YMfa/Pnzi+uedNJJxXqT6q6lv+SSS4r1NsfZO+HIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcD37ce7UU08t1ufMmVOsL1y4sFh//vnyLN2la8o//vjj4roPPfRQsX7DDTcU64sWLepY6/X//ebNm4v1K664oqfn70XX17PbfsT2AdvbJy2bbfsF229Ut6f1s1kA/Ted0/jHJB39tRyrJG2KiHMlbap+BzDEasMeEZslvX/U4mslravur5N0XZ/7AtBn3X42fm5EjEtSRIzbPrPTA22PSDr2JsYCjjONXwgTEaOSRiXeoAPa1O3Q237b8ySpuj3Qv5YANKHbsD8t6Zbq/i2Sft6fdgA0pfY03vYTki6XNMf2Xkk/kHS/pJ/Zvk3SHknfabJJlJXGm1esWFFct+667F6tX7++Y+25554rrvvoo48W63XXjD/55JPFeknd98avXLmy6+duS23YI2JZh9K3+twLgAbxcVkgCcIOJEHYgSQIO5AEYQeS4Kukh0DdZaTPPPNMsX7++ed3rJ1wQvnv+eHDh4v1tWvXFusjI819Erru8ty6YcXSv73u3/3hhx8W61u3bi3WhxFHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2PiiNc0vShg0bivUZM2YU6/PmzSvWd+zY0bE2a9as4rrLlnW6qHHCtm3bivVe1E2L/NhjjxXrdZfnlsbSX3zxxeK6t956a7F+LOLIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+Tffee2/H2k033VRcd8GCBT1te9++fcX6Aw880LE2NjZWXPf111/vqqfpKl2Tfs011xTXvfLKK3va9gcffNCxVtpnkrRnz56etj2MOLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs1fuueeeYr00lt7rOPrOnTuL9auvvrpYf/fdd3vafpNuvvnmjrU1a9Y0uu3rr7++Y23z5s2NbnsY1R7ZbT9i+4Dt7ZOWrbb9O9uvVT9XNdsmgF5N5zT+MUlLp1i+JiIurH7+s79tAei32rBHxGZJ7w+gFwAN6uUNujtsb61O80/r9CDbI7bHbJc/pA2gUd2G/UeSvibpQknjkn7Y6YERMRoRiyNicZfbAtAHXYU9IvZHxKGIOCzpx5Iu7m9bAPqtq7DbnvzdxtdL2t7psQCGQ+04u+0nJF0uaY7tvZJ+IOly2xdKCkm7Jd3eYI99ceeddxbrq1evbmzbW7ZsKdZHR0eL9SbH0evmhq/7/vS77767WO9ljvTS9eiS9PjjjxfrGcfSS2rDHhFTzSKwtoFeADSIj8sCSRB2IAnCDiRB2IEkCDuQhCNicBuzB7exo+zfv79YP/300xvb9llnnVWsHzx4sFhfvLj84cPbb+888nnZZZcV1z355JOL9brpouvY7lirmza57uueN27c2E1Lx72ImHKnc2QHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSTSjLMfOnSoWG9yP/Q6HnzRRRcV63PmzOlYK41zS9I777xTrI+Pjxfr9913X7Fe2n7dJaiffvppsY6pMc4OJEfYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzl4Z5H4YpLpx9kWLFhXrb731Vj/bwQAwzg4kR9iBJAg7kARhB5Ig7EAShB1IgrADSdTO4nq8uPHGG4v1nTt3FuvPPvtsx9qCBQuK6z788MPF+vLly4v1l156qVhfs2ZNxxrTFuOI2iO77fm2f2l7l+0dtr9XLZ9t+wXbb1S3pzXfLoBuTec0/jNJfxcR50n6c0krbJ8vaZWkTRFxrqRN1e8AhlRt2CNiPCJere5/JGmXpLMlXStpXfWwdZKua6pJAL37Qq/ZbS+U9HVJv5I0NyLGpYk/CLbP7LDOiKSR3toE0Ktph932LEnrJa2MiA/rLrA4IiJGJY1Wz3F8Xm0CHAOmNfRm+yRNBP0nEfFUtXi/7XlVfZ6kA820CKAfai9x9cQhfJ2k9yNi5aTl/yLpfyLifturJM2OiL+vea5j9shemrp4xowZxXXrLhM955xzivW6KZ35ymVM1ukS1+mcxn9D0s2Sttl+rVr2fUn3S/qZ7dsk7ZH0nX40CqAZtWGPiP+W1OkF+rf62w6ApvBxWSAJwg4kQdiBJAg7kARhB5JI81XSQBZ8lTSQHGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRRG3bb823/0vYu2ztsf69avtr272y/Vv1c1Xy7ALpVO0mE7XmS5kXEq7a/LOkVSddJ+itJH0fEv057Y0wSATSu0yQR05mffVzSeHX/I9u7JJ3d3/YANO0LvWa3vVDS1yX9qlp0h+2tth+xfVqHdUZsj9ke66lTAD2Z9lxvtmdJeknSP0fEU7bnSjooKST9kyZO9f+m5jk4jQca1uk0flpht32SpGclbYyIf5uivlDSsxHxpzXPQ9iBhnU9saNtS1oradfkoFdv3B1xvaTtvTYJoDnTeTf+m5L+S9I2SYerxd+XtEzShZo4jd8t6fbqzbzSc3FkBxrW02l8vxB2oHnMzw4kR9iBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii9gsn++ygpHcn/T6nWjaMhrW3Ye1Lordu9bO3BZ0KA72e/XMbt8ciYnFrDRQMa2/D2pdEb90aVG+cxgNJEHYgibbDPtry9kuGtbdh7Uuit24NpLdWX7MDGJy2j+wABoSwA0m0EnbbS23/2vabtle10UMntnfb3lZNQ93q/HTVHHoHbG+ftGy27Rdsv1HdTjnHXku9DcU03oVpxlvdd21Pfz7w1+y2T5T0G0nflrRX0suSlkXEzoE20oHt3ZIWR0TrH8CwfamkjyX9+5GptWw/IOn9iLi/+kN5WkT8w5D0tlpfcBrvhnrrNM34X6vFfdfP6c+70caR/WJJb0bE2xHxe0k/lXRtC30MvYjYLOn9oxZfK2lddX+dJv6zDFyH3oZCRIxHxKvV/Y8kHZlmvNV9V+hrINoI+9mSfjvp970arvneQ9IvbL9ie6TtZqYw98g0W9XtmS33c7TaabwH6ahpxodm33Uz/Xmv2gj7VFPTDNP43zci4s8k/aWkFdXpKqbnR5K+pok5AMcl/bDNZqppxtdLWhkRH7bZy2RT9DWQ/dZG2PdKmj/p969I2tdCH1OKiH3V7QFJGzTxsmOY7D8yg251e6Dlfv5fROyPiEMRcVjSj9XivqumGV8v6ScR8VS1uPV9N1Vfg9pvbYT9ZUnn2v6q7S9J+q6kp1vo43Nsz6zeOJHtmZKWaPimon5a0i3V/Vsk/bzFXv7AsEzj3WmacbW871qf/jwiBv4j6SpNvCP/lqR/bKOHDn39iaQt1c+OtnuT9IQmTuv+VxNnRLdJOl3SJklvVLezh6i3xzUxtfdWTQRrXku9fVMTLw23Snqt+rmq7X1X6Gsg+42PywJJ8Ak6IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUji/wCLUlKYMK7ZWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26d59b6c070>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the Pytorch version. \n",
    "\n",
    "It has a couple of particularly interesting args:\n",
    "\n",
    "- `drop_last` : drop the last incomplete batch \n",
    "-  `num_workers`: how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "\n",
    "train_dl = DataLoader(train_ds, bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, 128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()\n",
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4490, grad_fn=<NllLossBackward>), tensor(0.8611))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpreds = model(x_valid)\n",
    "loss,acc = loss_func(vpreds, y_valid), accuracy(vpreds, y_valid)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation set is a key component to training properly: it is the only real indication we have that the model learning something useful. \n",
    "\n",
    "Specifically, it signals to us whether or not the model is overfitting to the training data. \n",
    "\n",
    "If we were to simply watch the training accuracy we would mostly likely see a continual improvement, as the loss diminishes and the accuracy increases to nearly 100%. \n",
    "\n",
    "That only indicates the performance of the model on labelled data. If the model's learning algorithm is powerful and it has enough parameters it can memorize the training data. \n",
    "\n",
    "But then, during inference time, when it is used to make predictions on unseen data, i.e. generalize, it may do horribly. \n",
    "\n",
    "So let's build a more complete training loop that includes a validation error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: `model.train()` and `model.eval()` are used to turn on and off certain types of layers like Dropout and BatchNorm.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for xb, yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "\n",
    "        model.eval()\n",
    "        vloss=[]\n",
    "        acc=[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in valid_dl:\n",
    "                preds = model(xb)\n",
    "                vloss.append(loss_func(preds, yb))\n",
    "                acc.append(accuracy(preds, yb))\n",
    "        print('loss:', sum([i.item() for i in vloss])/len(valid_dl))\n",
    "        print('accuracy:', sum([i.item() for i in acc])/len(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.1706093008146633\n",
      "accuracy: 0.9481803797468354\n",
      "loss: 0.19026051393321045\n",
      "accuracy: 0.9396756329113924\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "fit(2, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our dataloaders, getting a model and optimizer, and training can be run in three lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.21858973362589185\n",
      "accuracy: 0.9353243670886076\n",
      "loss: 0.11040582522584856\n",
      "accuracy: 0.9675632911392406\n",
      "loss: 0.11853115729798999\n",
      "accuracy: 0.9622231012658228\n",
      "loss: 0.1047832053537846\n",
      "accuracy: 0.9698378164556962\n"
     ]
    }
   ],
   "source": [
    "get_dls(train_ds, valid_ds, 128)\n",
    "model, opt = get_model()\n",
    "fit(4, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03_minibatch_training.ipynb to exp\\nb_03.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 03_minibatch_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
